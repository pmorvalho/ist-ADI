{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 4: Supervised learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The LFW dataset\n",
    "\n",
    "The LFW (Labeled Faces in the Wild) data set is a collection of JPEG pictures of famous people collected over the internet. Each picture is centered on a single face and corresponds to a grayscale image of 62 $\\times$ 47 pixels. \n",
    "\n",
    "---\n",
    "\n",
    "We start by loading the dataset, selecting only those people in the dataset that appear in at least 50 pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load dataset\n",
    "lfw = datasets.fetch_lfw_people(min_faces_per_person=50)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Print sample digits\n",
    "for i in range(12): \n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    idx=list(lfw.target).index(i)\n",
    "    plt.imshow(lfw.images[idx], cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first activities, you will prepare the dataset, before running the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "From the LFW dataset, construct the training and test sets. The input data can be accessed as the attribute `data` in the dataset `lfw`; the corresponding output data can be accessed as the attribute `target` in `lfw`. To build the train and test sets, you can use the function `train_test_split` from the module `model_selection` of `scikit-learn`. Make sure that the test set corresponds to 10% of your data. \n",
    "\n",
    "**Note:** Don't forget to import `numpy` and the necessary modules from `scikit-learn`. Also, for reproducibility, initialize the seed of the `train_test_split` function to a fixed number.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, each point in the dataset is represented by the pixel information, which roughly corresponds to 3,000 features. In this activity, you will determine a small number of alternative features that manage to capture most of the relevant information contained in each picture but which provide a much more compact representation thereto. Such features correspond to the _principal components_ and can be computed through the function `PCA`, in the `decomposition` module of `scikit-learn`. \n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "* Run PCA on the training set. To do this, you should first fit the PCA model to the data and then use the resulting model to transform the data. For details, check the documentation for the function `PCA`.\n",
    "\n",
    "* To grasp how much of the information in the data is contained in the different components, plot the _cumulative explained variance_ (in percentage) as a function of the number of components. The explained variance can be accessed via the attribute `explained_variance_` of your model.\n",
    "\n",
    "**Note:** In general, before running PCA on some training set, you should _normalize_ the data to make sure that all inputs lie in the same range. In our case, since all pixels lie in the same range, normalization is not necessary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how 50 components explain almost 90\\% of the variance in the data. As such, it seems reasonable that we may rely only on those 50 components as features to represent our data. \n",
    "\n",
    "However, to clearly understand the implications of the adopted representation, you will now run an extensive test to investigate how the number of components may impact the performance of the classifier. \n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "Take the data in your training set and further split it in two sets, $D_T$ and $D_V$, where $D_T$ corresponds to $85\\%$ of the training data and $D_V$ to the remaining $15\\%$. You will use $D_T$ for training, and $D_V$ for validation. \n",
    "\n",
    "For $k\\in\\{10, 20, ..., 100\\}$,\n",
    "\n",
    "* Run PCA with $k$ components on the data in $D_T$\n",
    "* Transform the data in $D_T$ using the computed PCA\n",
    "* Train a logistic regression classifier on the transformed data, with $C=1000$\n",
    "* Compute the error in $D_T$ and in $D_V$\n",
    "\n",
    "Repeat the _whole process_ (including the split of $D_T$ and $D_V$) 30 times.\n",
    "\n",
    "** Note: ** The whole process may take a while, so don't despair. The logistic classifier can be accessed by importing `LogisticRegression` from `sklearn.linear_model`. To compute the error of a classifier, you can use the `accuracy_score` function from `sklearn.metrics`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Plot the average training and validation error from Activity 3 as a function of $k$. Comment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the homework, the logistic regression model is trained to minimize the cost function\n",
    "\n",
    "$$J(\\pi)=-\\frac{1}{N}\\sum_{n=1}^N\\log(\\pi(a_n\\mid x_n)).$$\n",
    "\n",
    "However, the logistic regression model in `scikit-learn` uses a slightly different cost function; this modified cost function includes a term that penalizes large values for the parameters $\\mathbf{w}$ and takes the general form\n",
    "\n",
    "$$J(\\pi)=\\frac{1}{2}\\|w\\|^2-\\frac{C}{N}\\sum_{n=1}^N\\log(\\pi(a_n\\mid x_n)).$$\n",
    "\n",
    "The first term is called a _regularization term_ and the constant $C$ expresses how much we are willing to pay (in terms of errors in the training set) to have small parameter vectors. In activity 3 you used a large value for $C$, indicating the algorithm that errors in the training set should be minimized.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Repeat Activity 3 but now setting $C=1$. Plot the results and comment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of the results, we can now safely train our classifier with a larger number of components (say, $k=80$) by setting $C=1$ to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "* Retrain your PCA model with $k=80$ components---now using all your training data;\n",
    "* Retrain the logistic regression model;\n",
    "* Compute the performance of the resulting model in the test data (don't forget to transform the test data).\n",
    "\n",
    "In particular, compute both the accuracy score and print the confusion matrix (which you can access from `sklearn.metrics`). Comment the obtained results in face of Activities 3-5.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
